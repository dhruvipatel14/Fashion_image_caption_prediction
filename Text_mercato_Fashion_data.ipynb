{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_mercato_Fashion_data",
      "provenance": [],
      "authorship_tag": "ABX9TyOH0RJPLBIjeuW5Ag5K/PJY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dhruvipatel14/Fashion_image_caption_prediction/blob/main/Text_mercato_Fashion_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gj8KC8hPrbSU",
        "outputId": "8d8c0931-977e-4abf-ea2c-3a9bc2c499dc"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0hA3aZTrupw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e48012e-ffe4-494a-cfb2-c0638758f523"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "pd.set_option('max_colwidth', -1) \r\n",
        "import string"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 797
        },
        "id": "8ihDRSOOwsRi",
        "outputId": "6c5ee51e-d146-4dfc-c1c8-d5d16e12c333"
      },
      "source": [
        "train_data = pd.read_csv('/content/drive/MyDrive/Text_mercato_fashion_data/train.csv')\r\n",
        "train_df = pd.DataFrame(train_data)\r\n",
        "test_data = pd.read_csv('/content/drive/MyDrive/Text_mercato_fashion_data/test.csv')\r\n",
        "test_df = pd.DataFrame(test_data)\r\n",
        "train_df.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Title</th>\n",
              "      <th>Description</th>\n",
              "      <th>Material</th>\n",
              "      <th>Pattern</th>\n",
              "      <th>Neckline</th>\n",
              "      <th>Image_Path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Peach Poly Crepe jumpsuit</td>\n",
              "      <td>This stylish foil print kurta from janasya is made of poly crepe and comes in an attractive peach color. It features 3/4 sleeve,round neck,a-line and it is calf length kurta that is suitable for casual occasions. Team it with matching leggings for a chic look.</td>\n",
              "      <td>Crepe</td>\n",
              "      <td>Printed</td>\n",
              "      <td>Round Neck</td>\n",
              "      <td>/images/pic_0.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Light Brown Bias Yoke Checks Top</td>\n",
              "      <td>This check pattern top by Work Label is crafted in cotton. Featuring a bias check at the yoke and straight check pattern in bottom half, a smart round Neckline, 3/4th sleeves, this mid hip length top offers a stylish &amp; comfortable fit. Style this top with a trouser/skirt and medium high heels for chic look at work. This top can also be styled with a pair of stud earrings and a pair your regular sneakers to attain a casual look.</td>\n",
              "      <td>Cotton</td>\n",
              "      <td>Checks</td>\n",
              "      <td>Round Neck</td>\n",
              "      <td>/images/pic_1.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Off White Geometric Straight Cotton Dobby Top With Skirt (Set of 2)</td>\n",
              "      <td>Featuring elegant printed details, this off white top and skirt set from Jaipur Kurti makes a statement addition to your casual wardrobe. Style this set with a pair of high heels and statement accessories to complete the look.</td>\n",
              "      <td>Viscose</td>\n",
              "      <td>Checks</td>\n",
              "      <td>Round Neck</td>\n",
              "      <td>/images/pic_2.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Blue Me Away Cape Top</td>\n",
              "      <td>Add an extra dose of style to your casual wardrobe with this elegant blue cape top from Twenty Dresses. Style With: A pair of black denims, colour block heels and statement earrings will complete this look!</td>\n",
              "      <td>Polyester</td>\n",
              "      <td>Solid/Plain</td>\n",
              "      <td>V-Neck</td>\n",
              "      <td>/images/pic_3.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Yellow On A High Gown</td>\n",
              "      <td>Yellow polyester georgette maxi dress. Polyester knit lining inside for comfort. Overlapping v neckline with gathers at bust. Flared sleeves with attached belt at the waist. Concealed zipper on the left.</td>\n",
              "      <td>Polyester</td>\n",
              "      <td>Solid/Plain</td>\n",
              "      <td>V-Neck</td>\n",
              "      <td>/images/pic_4.jpg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                 Title  ...         Image_Path\n",
              "0  Peach Poly Crepe jumpsuit                                            ...  /images/pic_0.jpg\n",
              "1  Light Brown Bias Yoke Checks Top                                     ...  /images/pic_1.jpg\n",
              "2  Off White Geometric Straight Cotton Dobby Top With Skirt (Set of 2)  ...  /images/pic_2.jpg\n",
              "3  Blue Me Away Cape Top                                                ...  /images/pic_3.jpg\n",
              "4  Yellow On A High Gown                                                ...  /images/pic_4.jpg\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "os3FqHlr955G"
      },
      "source": [
        "Extracting features from Image dataset using VGG"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnWCJb6m6ufV"
      },
      "source": [
        "# !unzip /content/drive/MyDrive/Text_mercato_fashion_data/train_image.zip -d /content/drive/MyDrive/Text_mercato_fashion_data/train_image"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USJFSUa4BerJ"
      },
      "source": [
        "# !unzip /content/drive/MyDrive/Text_mercato_fashion_data/test_data.zip -d /content/drive/MyDrive/Text_mercato_fashion_data/"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCxL_-TsB7HK"
      },
      "source": [
        "from os import listdir\r\n",
        "from pickle import dump\r\n",
        "from keras.applications.vgg16 import VGG16, preprocess_input\r\n",
        "from keras.preprocessing.image import load_img,img_to_array\r\n",
        "from keras.models import Model"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVj3I2m9DZTY"
      },
      "source": [
        "def extract_features(directory):\r\n",
        "  model = VGG16()\r\n",
        "  model = Model(inputs= model.inputs,outputs = model.layers[-2].output)\r\n",
        "  print(model.summary())\r\n",
        "\r\n",
        "  features = dict()\r\n",
        "  for name in listdir(directory):\r\n",
        "    filename = directory + '/' + name\r\n",
        "    img = load_img(filename,target_size=(224,224))\r\n",
        "    img = img_to_array(img)\r\n",
        "    img = img.reshape((1,img.shape[0],img.shape[1],img.shape[2]))\r\n",
        "    img = preprocess_input(img)\r\n",
        "    feature = model.predict(img,verbose = 0)\r\n",
        "    img_id = name.split('.')[0]\r\n",
        "    features[img_id] = feature\r\n",
        "  return features"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uC_S8WPnGTJR",
        "outputId": "475e412b-c3b6-42ee-f9e2-c47b6c72a914"
      },
      "source": [
        "# extract features from all images\r\n",
        "train_dict = '/content/drive/MyDrive/Text_mercato_fashion_data/train_image/train_image'\r\n",
        "test_dict = '/content/drive/MyDrive/Text_mercato_fashion_data/test_data'\r\n",
        "train_features = extract_features(train_dict)\r\n",
        "test_features = extract_features(test_dict)\r\n",
        "print('Extracted Train Features: %d' % len(train_features))\r\n",
        "print('Extracted Dev Features: %d' % len(test_features))\r\n",
        "# save to file\r\n",
        "dump(train_features, open('train_features.pkl', 'wb'))\r\n",
        "dump(test_features,open('test_features.pkl','wb'))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n",
            "553467904/553467096 [==============================] - 3s 0us/step\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "fc1 (Dense)                  (None, 4096)              102764544 \n",
            "_________________________________________________________________\n",
            "fc2 (Dense)                  (None, 4096)              16781312  \n",
            "=================================================================\n",
            "Total params: 134,260,544\n",
            "Trainable params: 134,260,544\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "fc1 (Dense)                  (None, 4096)              102764544 \n",
            "_________________________________________________________________\n",
            "fc2 (Dense)                  (None, 4096)              16781312  \n",
            "=================================================================\n",
            "Total params: 134,260,544\n",
            "Trainable params: 134,260,544\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Extracted Train Features: 400\n",
            "Extracted Dev Features: 100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KlhBCu5H1cV"
      },
      "source": [
        "Preparing Text data : Extracting and cleaning image discription from dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8p5Z5VXBgWk"
      },
      "source": [
        "# def clean_text(desc):\r\n",
        "#   table = str.maketrans('', '', string.punctuation)\r\n",
        "#   desc = desc.split()\r\n",
        "#   desc = [word.lower() for word in desc]\r\n",
        "#   desc = [w.translate(table) for w in desc]\r\n",
        "#   desc = [word for word in desc if len(word)>1]\r\n",
        "#   desc = [word for word in desc if word.isalpha()]\r\n",
        "#   return desc"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0IHr0GiG1Ry",
        "outputId": "f5e9c960-f596-4ed7-cf9c-3bc08be80a14"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('stopwords')\r\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8BxJhJ6GYaL"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\r\n",
        "from string import punctuation\r\n",
        "from nltk.corpus import stopwords \r\n",
        "import re\r\n",
        "stopword = set(stopwords.words('english') + list(punctuation))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15VL0zYAHnbY"
      },
      "source": [
        "# def tokens(text):\r\n",
        "#   \"List all the word tokens in a text.\"\r\n",
        "#   text = re.sub('[0-9]+', '', text)\r\n",
        "#   ptrn = re.compile('<.*?==>')\r\n",
        "#   text = re.sub(ptrn, '', text)\r\n",
        "#   text = text.lower()\r\n",
        "#   return text"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4l6NXlmG67D"
      },
      "source": [
        "# train_df['Description'] = train_df['Description'].apply(lambda x : tokens(x))\r\n",
        "# train_df['Description'] = train_df['Description'].apply(lambda x : word_tokenize(x))\r\n",
        "# train_df['Description'] = train_df['Description'].apply(lambda x : [item for item in x if item not in stopword])\r\n",
        "# train_df['Image_Path']= train_df['Image_Path'].apply(lambda s : s.split('/')[-1].split('.')[0])\r\n",
        "# # train_df.set_index(train_df['Image_Path'],inplace= True)\r\n",
        "# # train_df.drop('Image_Path',axis=1,inplace=True)\r\n",
        "# # description_mapping = train_df.to_dict('index')\r\n",
        "# # description_mapping\r\n",
        "\r\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQp1kErlPrEk"
      },
      "source": [
        "# train_df.set_index(train_df['Image_Path'],inplace= True)\r\n",
        "# train_df.drop('Image_Path',axis=1,inplace=True)\r\n",
        "# description_mapping = train_df.to_dict('index')\r\n",
        "# description_mapping"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtHmGphxT2e1"
      },
      "source": [
        "# d = train_df.set_index('Image_Path')['Description'].to_dict()\r\n",
        "# d"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzZ3JELMNfhR"
      },
      "source": [
        "# train_df"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WsPOeedhEcRi"
      },
      "source": [
        "# train_df['Description'] = train_df['Description'].apply(lambda x : clean_text(x))\r\n",
        "# train_df['Description']"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsK2omrpN3to"
      },
      "source": [
        "def tokens(text):\r\n",
        "  \"List all the word tokens in a text.\"\r\n",
        "  text = re.sub('[0-9]+', '', text)\r\n",
        "  ptrn = re.compile('<.*?==>')\r\n",
        "  text = re.sub(ptrn, '', text)\r\n",
        "  text = text.lower()\r\n",
        "  return text\r\n",
        "\r\n",
        "def description_map(df):\r\n",
        "  desc_df = df[['Description','Image_Path']]\r\n",
        "  desc_df['Image_Path']= desc_df['Image_Path'].apply(lambda s : s.split('/')[-1].split('.')[0])\r\n",
        "  desc_df['Description'] = desc_df['Description'].apply(lambda x : tokens(x))\r\n",
        "  desc_df['Description'] = desc_df['Description'].apply(lambda x : word_tokenize(x))\r\n",
        "  desc_df['Description'] = desc_df['Description'].apply(lambda x : [item for item in x if item not in stopword])\r\n",
        "  # desc_df.set_index(desc_df['Image_Path'],inplace= True)\r\n",
        "  # desc_df.drop('Image_Path',axis=1,inplace=True)\r\n",
        "  # description_mapping = desc_df.to_dict()\r\n",
        "  description_mapping = desc_df.set_index('Image_Path')['Description'].to_dict()\r\n",
        "  return description_mapping\r\n",
        "\r\n",
        "def to_vocabulary(descriptions):\r\n",
        "\tall_desc = set()\r\n",
        "\tfor key in descriptions.keys():\r\n",
        "\t\t[all_desc.update(d.split()) for d in descriptions[key]]\r\n",
        "\treturn all_desc\r\n",
        " \r\n",
        "# save descriptions to file, one per line\r\n",
        "def save_descriptions(descriptions, filename):\r\n",
        "\tlines = list()\r\n",
        "\tfor key, desc_list in descriptions.items():\r\n",
        "\t\tfor desc in desc_list:\r\n",
        "\t\t\tlines.append(key + ' ' + desc)\r\n",
        "\tdata = '\\n'.join(lines)\r\n",
        "\tfile = open(filename, 'w')\r\n",
        "\tfile.write(data)\r\n",
        "\tfile.close()"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yt8iTmEX9Pjv",
        "outputId": "fe21f43a-3ef0-4745-9217-9bdb6eb9a626"
      },
      "source": [
        "training_mapping = description_map(train_df)\r\n",
        "testing_mapping = description_map(test_df)\r\n",
        "\r\n",
        "train_vocabulary = to_vocabulary(training_mapping)\r\n",
        "print('Vocabulary Size: %d' % len(train_vocabulary))\r\n",
        "# save to file\r\n",
        "save_descriptions(training_mapping, 'train_descriptions.txt')\r\n",
        "save_descriptions(testing_mapping,'test_descriptions.txt')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary Size: 1365\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if sys.path[0] == '':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjFVH7f5qRmS"
      },
      "source": [
        "Preparing data for model training\r\n",
        "text data and image data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Uuy4mikuH18"
      },
      "source": [
        "from numpy import array,argmax\r\n",
        "import pickle\r\n",
        "from keras.preprocessing.text import Tokenizer\r\n",
        "from keras.preprocessing.sequence import pad_sequences\r\n",
        "from keras.utils import to_categorical, plot_model\r\n",
        "from keras.models import Model\r\n",
        "from keras.layers import Input,Dense,LSTM,Embedding,Dropout\r\n",
        "from keras.layers.merge import add\r\n",
        "from keras.callbacks import ModelCheckpoint\r\n",
        "\r\n",
        "from keras.models import load_model\r\n",
        "from nltk.translate.bleu_score import corpus_bleu"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4Z9GV-fqlTL"
      },
      "source": [
        "# desc = dict()\r\n",
        "# for key,val in training_mapping.items():\r\n",
        "#   if key not in desc:\r\n",
        "#     desc[key] = list()\r\n",
        "#   seq = 'startseq ' + ' '.join(val) + ' endseq'\r\n",
        "#   desc[key].append(seq)\r\n",
        "# print(desc)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-XcLKGhVT8U"
      },
      "source": [
        "def load_clean_descriptions(mapping):\r\n",
        "  desc = dict()\r\n",
        "  for key,val in mapping.items():\r\n",
        "    if key not in desc:\r\n",
        "      desc[key] = list()\r\n",
        "    seq = 'startseq ' + ' '.join(val) + ' endseq'\r\n",
        "    desc[key].append(seq)\r\n",
        "  return desc\r\n",
        "\r\n",
        "# load photo features\r\n",
        "def load_photo_features(filename):\r\n",
        "\tfeatures = pickle.load(open(filename, 'rb'))\r\n",
        "\treturn features\r\n",
        "\r\n",
        "def to_lines(descriptions):\r\n",
        "\tall_desc = list()\r\n",
        "\tfor key in descriptions.keys():\r\n",
        "\t\t[all_desc.append(d) for d in descriptions[key]]\r\n",
        "\treturn all_desc\r\n",
        "\r\n",
        "# fit a tokenizer given caption descriptions\r\n",
        "def create_tokenizer(descriptions):\r\n",
        "\tlines = to_lines(descriptions)\r\n",
        "\ttokenizer = Tokenizer()\r\n",
        "\ttokenizer.fit_on_texts(lines)\r\n",
        "\treturn tokenizer\r\n",
        "\r\n",
        "# calculate the length of the description with the most words\r\n",
        "def max_length(descriptions):\r\n",
        "\tlines = to_lines(descriptions)\r\n",
        "\treturn max(len(d.split()) for d in lines)\r\n",
        " \r\n",
        "\r\n",
        "# create sequences of images, input sequences and output words for an image\r\n",
        "def create_sequences(tokenizer, max_length, descriptions, photos, vocab_size):\r\n",
        "\tX1, X2, y = list(), list(), list()\r\n",
        "\t# walk through each image identifier\r\n",
        "\tfor key, desc_list in descriptions.items():\r\n",
        "\t\t# walk through each description for the image\r\n",
        "\t\tfor desc in desc_list:\r\n",
        "\t\t\t# encode the sequence\r\n",
        "\t\t\tseq = tokenizer.texts_to_sequences([desc])[0]\r\n",
        "\t\t\t# split one sequence into multiple X,y pairs\r\n",
        "\t\t\tfor i in range(1, len(seq)):\r\n",
        "\t\t\t\t# split into input and output pair\r\n",
        "\t\t\t\tin_seq, out_seq = seq[:i], seq[i]\r\n",
        "\t\t\t\t# pad input sequence\r\n",
        "\t\t\t\tin_seq = pad_sequences([in_seq], maxlen=max_length)[0]\r\n",
        "\t\t\t\t# encode output sequence\r\n",
        "\t\t\t\tout_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\r\n",
        "\t\t\t\t# store\r\n",
        "\t\t\t\tX1.append(photos[key][0])\r\n",
        "\t\t\t\tX2.append(in_seq)\r\n",
        "\t\t\t\ty.append(out_seq)\r\n",
        "\treturn array(X1), array(X2), array(y)\r\n",
        "\r\n",
        "# define the captioning model\r\n",
        "def define_model(vocab_size, max_length):\r\n",
        "\t# feature extractor model\r\n",
        "\tinputs1 = Input(shape=(4096,))\r\n",
        "\tfe1 = Dropout(0.5)(inputs1)\r\n",
        "\tfe2 = Dense(256, activation='relu')(fe1)\r\n",
        "\t# sequence model\r\n",
        "\tinputs2 = Input(shape=(max_length,))\r\n",
        "\tse1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\r\n",
        "\tse2 = Dropout(0.5)(se1)\r\n",
        "\tse3 = LSTM(256)(se2)\r\n",
        "\t# decoder model\r\n",
        "\tdecoder1 = add([fe2, se3])\r\n",
        "\tdecoder2 = Dense(256, activation='relu')(decoder1)\r\n",
        "\toutputs = Dense(vocab_size, activation='softmax')(decoder2)\r\n",
        "\t# tie it together [image, seq] [word]\r\n",
        "\tmodel = Model(inputs=[inputs1, inputs2], outputs=outputs)\r\n",
        "\t# compile model\r\n",
        "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam')\r\n",
        "\t# summarize model\r\n",
        "\tmodel.summary()\r\n",
        "\tplot_model(model, to_file='model.png', show_shapes=True)\r\n",
        "\treturn model\r\n",
        "\r\n",
        "# data generator, intended to be used in a call to model.fit_generator()\r\n",
        "def data_generator(descriptions, photos, tokenizer, max_length, vocab_size):\r\n",
        "\t# loop for ever over images\r\n",
        "\twhile 1:\r\n",
        "\t\tfor key, desc_list in descriptions.items():\r\n",
        "\t\t\t# retrieve the photo feature\r\n",
        "\t\t\tphoto = photos[key][0]\r\n",
        "\t\t\tin_img, in_seq, out_word = create_sequences(tokenizer, max_length, desc_list, photo, vocab_size)\r\n",
        "\t\t\tyield [in_img, in_seq], out_word"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVmEJX0o6zmp"
      },
      "source": [
        "Training sequences\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xoI38i2C2OJs",
        "outputId": "917710d9-d002-4bde-9093-351ebbaa06f6"
      },
      "source": [
        "train_descriptions = load_clean_descriptions(training_mapping)\r\n",
        "print('Descriptions: train=%d' % len(train_descriptions))\r\n",
        "# photo features\r\n",
        "train_features = load_photo_features('train_features.pkl')\r\n",
        "print('Photos: train=%d' % len(train_features))\r\n",
        "# prepare tokenizer\r\n",
        "tokenizer = create_tokenizer(train_descriptions)\r\n",
        "vocab_size = len(tokenizer.word_index) + 1\r\n",
        "print('Vocabulary Size: %d' % vocab_size)\r\n",
        "# determine the maximum sequence length\r\n",
        "max_length = max_length(train_descriptions)\r\n",
        "# prepare sequences\r\n",
        "X1train, X2train, ytrain = create_sequences(tokenizer, max_length, train_descriptions, train_features, vocab_size)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Descriptions: train=400\n",
            "Photos: train=400\n",
            "Vocabulary Size: 1305\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-RhtW7Y657c"
      },
      "source": [
        "Dev Seqences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQT4AyCe65Mo",
        "outputId": "8e7b48b5-0a7f-4d4b-f6ca-c25a5148dc0f"
      },
      "source": [
        "# dev dataset\r\n",
        " \r\n",
        "test_descriptions = load_clean_descriptions(testing_mapping)\r\n",
        "print('Descriptions: test=%d' % len(test_descriptions))\r\n",
        "# photo features\r\n",
        "test_features = load_photo_features('test_features.pkl')\r\n",
        "print('Photos: test=%d' % len(test_features))\r\n",
        "# prepare sequences\r\n",
        "X1test, X2test, ytest = create_sequences(tokenizer, max_length, test_descriptions, test_features, vocab_size)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Descriptions: test=100\n",
            "Photos: test=100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2IrrC8P2h2x"
      },
      "source": [
        "Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KcOHhxdo2lbZ",
        "outputId": "52f6ac67-77f6-4b03-de27-f72b1413158b"
      },
      "source": [
        "# define the model\r\n",
        "model = define_model(vocab_size, max_length)\r\n",
        "# define checkpoint callback\r\n",
        "filepath = 'model-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5'\r\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\r\n",
        "# fit model\r\n",
        "model.fit([X1train, X2train], ytrain, epochs=20, verbose=2, callbacks=[checkpoint], validation_data=([X1test, X2test], ytest))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_4 (InputLayer)            [(None, 89)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_3 (InputLayer)            [(None, 4096)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 89, 256)      334080      input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 4096)         0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 89, 256)      0           embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 256)          1048832     dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     (None, 256)          525312      dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, 256)          0           dense[0][0]                      \n",
            "                                                                 lstm[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 256)          65792       add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 1305)         335385      dense_1[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 2,309,401\n",
            "Trainable params: 2,309,401\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/20\n",
            "309/309 - 149s - loss: 6.0741 - val_loss: 5.4373\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 5.43732, saving model to model-ep001-loss6.074-val_loss5.437.h5\n",
            "Epoch 2/20\n",
            "309/309 - 151s - loss: 5.2844 - val_loss: 4.9938\n",
            "\n",
            "Epoch 00002: val_loss improved from 5.43732 to 4.99377, saving model to model-ep002-loss5.284-val_loss4.994.h5\n",
            "Epoch 3/20\n",
            "309/309 - 151s - loss: 4.6950 - val_loss: 4.7246\n",
            "\n",
            "Epoch 00003: val_loss improved from 4.99377 to 4.72459, saving model to model-ep003-loss4.695-val_loss4.725.h5\n",
            "Epoch 4/20\n",
            "309/309 - 155s - loss: 4.1878 - val_loss: 4.5754\n",
            "\n",
            "Epoch 00004: val_loss improved from 4.72459 to 4.57537, saving model to model-ep004-loss4.188-val_loss4.575.h5\n",
            "Epoch 5/20\n",
            "309/309 - 152s - loss: 3.6956 - val_loss: 4.5287\n",
            "\n",
            "Epoch 00005: val_loss improved from 4.57537 to 4.52868, saving model to model-ep005-loss3.696-val_loss4.529.h5\n",
            "Epoch 6/20\n",
            "309/309 - 152s - loss: 3.2414 - val_loss: 4.6445\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 4.52868\n",
            "Epoch 7/20\n",
            "309/309 - 150s - loss: 2.8152 - val_loss: 4.6858\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 4.52868\n",
            "Epoch 8/20\n",
            "309/309 - 153s - loss: 2.4378 - val_loss: 4.9377\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 4.52868\n",
            "Epoch 9/20\n",
            "309/309 - 150s - loss: 2.1178 - val_loss: 5.0432\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 4.52868\n",
            "Epoch 10/20\n",
            "309/309 - 150s - loss: 1.8252 - val_loss: 5.4677\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 4.52868\n",
            "Epoch 11/20\n",
            "309/309 - 149s - loss: 1.5497 - val_loss: 5.7768\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 4.52868\n",
            "Epoch 12/20\n",
            "309/309 - 153s - loss: 1.3394 - val_loss: 5.9218\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 4.52868\n",
            "Epoch 13/20\n",
            "309/309 - 149s - loss: 1.1451 - val_loss: 6.3636\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 4.52868\n",
            "Epoch 14/20\n",
            "309/309 - 152s - loss: 0.9665 - val_loss: 6.6875\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 4.52868\n",
            "Epoch 15/20\n",
            "309/309 - 153s - loss: 0.8176 - val_loss: 6.9631\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 4.52868\n",
            "Epoch 16/20\n",
            "309/309 - 156s - loss: 0.6927 - val_loss: 7.2632\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 4.52868\n",
            "Epoch 17/20\n",
            "309/309 - 153s - loss: 0.5818 - val_loss: 7.5157\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 4.52868\n",
            "Epoch 18/20\n",
            "309/309 - 153s - loss: 0.5081 - val_loss: 7.7932\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 4.52868\n",
            "Epoch 19/20\n",
            "309/309 - 156s - loss: 0.4282 - val_loss: 8.0771\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 4.52868\n",
            "Epoch 20/20\n",
            "309/309 - 158s - loss: 0.3654 - val_loss: 8.2745\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 4.52868\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7ff8fd491d68>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecnliyAtB2mr"
      },
      "source": [
        "Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plYMgL4e4nCR"
      },
      "source": [
        "# map an integer to a word\r\n",
        "def word_for_id(integer, tokenizer):\r\n",
        "\tfor word, index in tokenizer.word_index.items():\r\n",
        "\t\tif index == integer:\r\n",
        "\t\t\treturn word\r\n",
        "\treturn None\r\n",
        "\r\n",
        "# generate a description for an image\r\n",
        "def generate_desc(model, tokenizer, photo, max_length):\r\n",
        "  in_text = 'startseq'\r\n",
        "  for i in range(max_length):\r\n",
        "    sequence = tokenizer.texts_to_sequences([in_text])[0]\r\n",
        "    sequence = pad_sequences([sequence],maxlen = max_length)\r\n",
        "    yhat = model.predict([photo,sequence],verbose=0)\r\n",
        "    yhat = argmax(yhat)\r\n",
        "    word = word_for_id(yhat,tokenizer)\r\n",
        "    if word is None:\r\n",
        "      break\r\n",
        "      in_text += ' ' + word\r\n",
        "      if word == 'endseq':\r\n",
        "        break\r\n",
        "  return in_text\r\n",
        "\r\n",
        "\r\n",
        "# evaluate the skill of the model\r\n",
        "def evaluate_model(model, descriptions, photos, tokenizer, max_length):\r\n",
        "\tactual, predicted = list(), list()\r\n",
        "\t# step over the whole set\r\n",
        "\tfor key, desc_list in descriptions.items():\r\n",
        "\t\t# generate description\r\n",
        "\t\tyhat = generate_desc(model, tokenizer, photos[key], max_length)\r\n",
        "\t\t# store actual and predicted\r\n",
        "\t\treferences = [d.split() for d in desc_list]\r\n",
        "\t\tactual.append(references)\r\n",
        "\t\tpredicted.append(yhat.split())\r\n",
        "\t# calculate BLEU score\r\n",
        "\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\r\n",
        "\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\r\n",
        "\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\r\n",
        "\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YonxLy4RDcil",
        "outputId": "cd33b2ae-7d33-417d-d837-15b240bb7d9c"
      },
      "source": [
        "evaluate_model(model, test_descriptions, test_features, tokenizer, max_length)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BLEU-1: 0.000000\n",
            "BLEU-2: 0.000000\n",
            "BLEU-3: 0.000000\n",
            "BLEU-4: 0.000000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TB0FbAtc4eqS"
      },
      "source": [
        "dump(tokenizer, open('tokenizer.pkl', 'wb'))"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qg-QDyiF4uGF"
      },
      "source": [
        "Generating caption on unseen image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UwFWb0N74tRi",
        "outputId": "4f760ae1-d948-468e-ade4-689a0d3156c6"
      },
      "source": [
        "from pickle import load\r\n",
        "from numpy import argmax\r\n",
        "from keras.preprocessing.sequence import pad_sequences\r\n",
        "from keras.applications.vgg16 import VGG16\r\n",
        "from keras.preprocessing.image import load_img\r\n",
        "from keras.preprocessing.image import img_to_array\r\n",
        "from keras.applications.vgg16 import preprocess_input\r\n",
        "from keras.models import Model\r\n",
        "from keras.models import load_model\r\n",
        "\r\n",
        "# extract features from each photo in the directory\r\n",
        "def extract_features(filename):\r\n",
        "  model = VGG16()\r\n",
        "  model = Model(inputs=model.inputs, outputs=model.layers[-2].output)\r\n",
        "  image = load_img(filename,target_size=(224,224))\r\n",
        "  image = img_to_array(image)\r\n",
        "  image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\r\n",
        "  image = preprocess_input(image)\r\n",
        "  feature = model.predict(image, verbose=0)\r\n",
        "  return feature\r\n",
        "\r\n",
        "# map an integer to a word\r\n",
        "def word_for_id(integer, tokenizer):\r\n",
        "\tfor word, index in tokenizer.word_index.items():\r\n",
        "\t\tif index == integer:\r\n",
        "\t\t\treturn word\r\n",
        "\treturn None\r\n",
        "\r\n",
        "# generate a description for an image\r\n",
        "def generate_desc(model, tokenizer, photo, max_length):\r\n",
        "\tin_text = 'startseq'\r\n",
        "\tfor i in range(max_length):\r\n",
        "\t\tsequence = tokenizer.texts_to_sequences([in_text])[0]\r\n",
        "\t\tsequence = pad_sequences([sequence], maxlen=max_length)\r\n",
        "\t\tyhat = model.predict([photo,sequence], verbose=0)\r\n",
        "\t\tyhat = argmax(yhat)\r\n",
        "\t\tword = word_for_id(yhat, tokenizer)\r\n",
        "\t\tif word is None:\r\n",
        "\t\t\tbreak\r\n",
        "\t\tin_text += ' ' + word\r\n",
        "\t\tif word == 'endseq':\r\n",
        "\t\t\tbreak\r\n",
        "\treturn in_text\r\n",
        "\r\n",
        "# load the tokenizer\r\n",
        "tokenizer = load(open('/content/tokenizer.pkl', 'rb'))\r\n",
        "# pre-define the max sequence length (from training)\r\n",
        "max_length = 89\r\n",
        "# load the model\r\n",
        "model = load_model('/content/model-ep005-loss3.696-val_loss4.529.h5')\r\n",
        "# load and prepare the photograph\r\n",
        "photo = extract_features('images.jpg')\r\n",
        "# generate description\r\n",
        "description = generate_desc(model, tokenizer, photo, max_length)\r\n",
        "print(description)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "startseq black polyester crepe jumpsuit sleeveless v neckline elasticated back zipper zipper fastening ease endseq\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}